# RF-based Token Compression Configuration

experiment:
  name: "rf_vision_token_compression"
  seed: 42
  use_wandb: true
  wandb_project: "rf-vision-token-compression"
  wandb_entity: medicalissues  # Set your wandb entity here

# Model settings
model:
  clip:
    model_name: "openai/clip-vit-large-patch14-336" #-336
    freeze: true
    image_size: 336

  compressor:
    hidden_dim: 1024  # CLIP ViT-L token dimension

# Loss settings
loss:
  sinkhorn_ot:
    epsilon: 0.1      # Entropic regularization parameter (smaller = closer to exact OT, but slower)
    max_iter: 100     # Maximum number of Sinkhorn iterations
    threshold: 1e-3   # Convergence threshold for stopping criterion
    normalize: true   # Whether to normalize the cost by the number of samples

  rf_cosine_similarity:
    # Loss = 1 - mean_similarity (range: 0 to 2)
    # No additional parameters needed

  weights:
    sinkhorn_ot: 1.0  # Weight for Sinkhorn OT loss
    cosine: 1.0       # Weight for Cosine Similarity loss

# Training settings
training:
  epochs: 100
  batch_size: 128
  learning_rate: 1e-4

  # Optimizer settings (AdamW)
  optimizer:
    betas: [0.9, 0.999]  # Vision Transformer standard (was [0.0, 0.9])
    weight_decay: 0.05   # Regularization for 9-26M params (was 0.0)
    eps: 1e-8           # Numerical stability

  # Learning rate scheduler with cosine annealing
  scheduler:
    use: true           # Enable scheduler for better convergence
    type: "cosine"
    warmup_unit: "epoch"  # Choose "epoch" or "step" for warmup
    warmup_epochs: 3    # Number of warmup epochs (used when warmup_unit="epoch")
    warmup_steps: 1000  # Number of warmup steps (used when warmup_unit="step")
    min_lr: 1e-6

# Compression settings - Choose one of 4 supported configurations:
# Uses Partial Convolution instead of zero padding for better boundary handling
# 1. 24 → 12: kernel=3, stride=2, padding=1, RF=3×3 (9.59M params)
# 2. 24 → 8:  kernel=3, stride=3, padding=0, RF=3×3 (9.50M params)
# 3. 16 → 12: kernel=5, stride=1, padding=0, RF=5×5 (26.36M params)
# 4. 16 → 8:  kernel=3, stride=2, padding=1, RF=3×3 (9.50M params)
compression:
  input_grid_size: 24   # Input token grid size: 16 or 24
  output_grid_size: 8   # Output token grid size: 8 or 12

# Data settings
data:
  dataset_type: "coco"  # Choose "imagenet" or "coco"

  # ImageNet settings
  imagenet_root: "/data/ImageNet"  # UPDATE THIS PATH

  # COCO settings
  coco_root: "/data/codo"  # UPDATE THIS PATH
  coco_annotation_train: null  # Default: {coco_root}/annotations/instances_train2017.json
  coco_annotation_val: null    # Default: {coco_root}/annotations/instances_val2017.json

  # Common settings
  num_workers: 24
  pin_memory: true
  use_subset: false  # Set to true for quick testing
  subset_size: 1000

# Hardware settings
hardware:
  cuda_device: 0  # CUDA device number
  mixed_precision: true  # Use automatic mixed precision for 1.5-2x speedup
  compile: true  # Use torch.compile (requires PyTorch 2.0+)

# Checkpoint settings
checkpoint:
  save_dir: "./checkpoints_rf"  # Will be auto-suffixed with config (e.g., checkpoints_rf/24to8)
  save_frequency: 5  # Save every N epochs
  keep_last_n: 3  # Keep only last N checkpoints

# Validation settings
validation:
  frequency: 1  # Validate every N epochs
  save_rf_visualizations: true  # Save RF reconstruction visualizations
  visualize_rf_indices: [0, 17, 35]  # Which RFs to visualize (adjust based on output_grid_size^2)
  compute_detailed_stats: true  # Compute per-RF statistics

# Logging settings
logging:
  log_frequency: 1  # Log to wandb every N steps
  print_frequency: 10  # Print to console every N steps
